----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 64, 32, 32]           1,792
              ReLU-2           [-1, 64, 32, 32]               0
            Conv2d-3           [-1, 64, 32, 32]          36,928
              ReLU-4           [-1, 64, 32, 32]               0
         MaxPool2d-5           [-1, 64, 16, 16]               0
            Conv2d-6          [-1, 128, 16, 16]          73,856
              ReLU-7          [-1, 128, 16, 16]               0
            Conv2d-8          [-1, 128, 16, 16]         147,584
              ReLU-9          [-1, 128, 16, 16]               0
        MaxPool2d-10            [-1, 128, 8, 8]               0
           Conv2d-11            [-1, 256, 8, 8]         295,168
             ReLU-12            [-1, 256, 8, 8]               0
           Conv2d-13            [-1, 256, 8, 8]         590,080
             ReLU-14            [-1, 256, 8, 8]               0
           Conv2d-15            [-1, 256, 8, 8]         590,080
             ReLU-16            [-1, 256, 8, 8]               0
        MaxPool2d-17            [-1, 256, 4, 4]               0
           Conv2d-18            [-1, 512, 4, 4]       1,180,160
             ReLU-19            [-1, 512, 4, 4]               0
           Conv2d-20            [-1, 512, 4, 4]       2,359,808
             ReLU-21            [-1, 512, 4, 4]               0
           Conv2d-22            [-1, 512, 4, 4]       2,359,808
             ReLU-23            [-1, 512, 4, 4]               0
        MaxPool2d-24            [-1, 512, 2, 2]               0
           Conv2d-25            [-1, 512, 2, 2]       2,359,808
             ReLU-26            [-1, 512, 2, 2]               0
           Conv2d-27            [-1, 512, 2, 2]       2,359,808
             ReLU-28            [-1, 512, 2, 2]               0
           Conv2d-29            [-1, 512, 2, 2]       2,359,808
             ReLU-30            [-1, 512, 2, 2]               0
        MaxPool2d-31            [-1, 512, 1, 1]               0
          Dropout-32                  [-1, 512]               0
           Linear-33                  [-1, 512]         262,656
             ReLU-34                  [-1, 512]               0
          Dropout-35                  [-1, 512]               0
           Linear-36                  [-1, 512]         262,656
             ReLU-37                  [-1, 512]               0
           Linear-38                   [-1, 10]           5,130
================================================================
Total params: 15,245,130
Trainable params: 15,245,130
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.01
Forward/backward pass size (MB): 4.48
Params size (MB): 58.16
Estimated Total Size (MB): 62.65
----------------------------------------------------------------
start converting conv layer
torch.Size([64, 3, 3, 3])
torch.Size([64, 8, 3, 3])
torch.Size([128, 8, 3, 3])
torch.Size([128, 8, 3, 3])
torch.Size([256, 8, 3, 3])
torch.Size([256, 8, 3, 3])
torch.Size([256, 8, 3, 3])
torch.Size([512, 8, 3, 3])
torch.Size([512, 8, 3, 3])
torch.Size([512, 8, 3, 3])
torch.Size([512, 8, 3, 3])
torch.Size([512, 8, 3, 3])
torch.Size([512, 8, 3, 3])
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 64, 32, 32]           1,792
              ReLU-2           [-1, 64, 32, 32]               0
            Conv2d-3           [-1, 64, 32, 32]          36,928
              ReLU-4           [-1, 64, 32, 32]               0
         MaxPool2d-5           [-1, 64, 16, 16]               0
            Conv2d-6          [-1, 128, 16, 16]          73,856
              ReLU-7          [-1, 128, 16, 16]               0
            Conv2d-8          [-1, 128, 16, 16]         147,584
              ReLU-9          [-1, 128, 16, 16]               0
        MaxPool2d-10            [-1, 128, 8, 8]               0
           Conv2d-11            [-1, 256, 8, 8]         295,168
             ReLU-12            [-1, 256, 8, 8]               0
           Conv2d-13            [-1, 256, 8, 8]         590,080
             ReLU-14            [-1, 256, 8, 8]               0
           Conv2d-15            [-1, 256, 8, 8]         590,080
             ReLU-16            [-1, 256, 8, 8]               0
        MaxPool2d-17            [-1, 256, 4, 4]               0
           Conv2d-18            [-1, 512, 4, 4]       1,180,160
             ReLU-19            [-1, 512, 4, 4]               0
           Conv2d-20            [-1, 512, 4, 4]       2,359,808
             ReLU-21            [-1, 512, 4, 4]               0
           Conv2d-22            [-1, 512, 4, 4]       2,359,808
             ReLU-23            [-1, 512, 4, 4]               0
        MaxPool2d-24            [-1, 512, 2, 2]               0
           Conv2d-25            [-1, 512, 2, 2]       2,359,808
             ReLU-26            [-1, 512, 2, 2]               0
           Conv2d-27            [-1, 512, 2, 2]       2,359,808
             ReLU-28            [-1, 512, 2, 2]               0
           Conv2d-29            [-1, 512, 2, 2]       2,359,808
             ReLU-30            [-1, 512, 2, 2]               0
        MaxPool2d-31            [-1, 512, 1, 1]               0
          Dropout-32                  [-1, 512]               0
           Linear-33                  [-1, 512]         262,656
             ReLU-34                  [-1, 512]               0
          Dropout-35                  [-1, 512]               0
           Linear-36                  [-1, 512]         262,656
             ReLU-37                  [-1, 512]               0
           Linear-38                   [-1, 10]           5,130
================================================================
Total params: 15,245,130
Trainable params: 15,245,130
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.01
Forward/backward pass size (MB): 4.48
Params size (MB): 58.16
Estimated Total Size (MB): 62.65
----------------------------------------------------------------
start converting conv layer
torch.Size([64, 3, 3, 3])
torch.Size([64, 8, 3, 3])
torch.Size([128, 8, 3, 3])
torch.Size([128, 8, 3, 3])
torch.Size([256, 8, 3, 3])
torch.Size([256, 8, 3, 3])
torch.Size([256, 8, 3, 3])
torch.Size([512, 8, 3, 3])
torch.Size([512, 8, 3, 3])
torch.Size([512, 8, 3, 3])
torch.Size([512, 8, 3, 3])
torch.Size([512, 8, 3, 3])
torch.Size([512, 8, 3, 3])
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 64, 32, 32]           1,792
              ReLU-2           [-1, 64, 32, 32]               0
            Conv2d-3           [-1, 64, 32, 32]          36,928
              ReLU-4           [-1, 64, 32, 32]               0
         MaxPool2d-5           [-1, 64, 16, 16]               0
            Conv2d-6          [-1, 128, 16, 16]          73,856
              ReLU-7          [-1, 128, 16, 16]               0
            Conv2d-8          [-1, 128, 16, 16]         147,584
              ReLU-9          [-1, 128, 16, 16]               0
        MaxPool2d-10            [-1, 128, 8, 8]               0
           Conv2d-11            [-1, 256, 8, 8]         295,168
             ReLU-12            [-1, 256, 8, 8]               0
           Conv2d-13            [-1, 256, 8, 8]         590,080
             ReLU-14            [-1, 256, 8, 8]               0
           Conv2d-15            [-1, 256, 8, 8]         590,080
             ReLU-16            [-1, 256, 8, 8]               0
        MaxPool2d-17            [-1, 256, 4, 4]               0
           Conv2d-18            [-1, 512, 4, 4]       1,180,160
             ReLU-19            [-1, 512, 4, 4]               0
           Conv2d-20            [-1, 512, 4, 4]       2,359,808
             ReLU-21            [-1, 512, 4, 4]               0
           Conv2d-22            [-1, 512, 4, 4]       2,359,808
             ReLU-23            [-1, 512, 4, 4]               0
        MaxPool2d-24            [-1, 512, 2, 2]               0
           Conv2d-25            [-1, 512, 2, 2]       2,359,808
             ReLU-26            [-1, 512, 2, 2]               0
           Conv2d-27            [-1, 512, 2, 2]       2,359,808
             ReLU-28            [-1, 512, 2, 2]               0
           Conv2d-29            [-1, 512, 2, 2]       2,359,808
             ReLU-30            [-1, 512, 2, 2]               0
        MaxPool2d-31            [-1, 512, 1, 1]               0
          Dropout-32                  [-1, 512]               0
           Linear-33                  [-1, 512]         262,656
             ReLU-34                  [-1, 512]               0
          Dropout-35                  [-1, 512]               0
           Linear-36                  [-1, 512]         262,656
             ReLU-37                  [-1, 512]               0
           Linear-38                   [-1, 10]           5,130
================================================================
Total params: 15,245,130
Trainable params: 15,245,130
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.01
Forward/backward pass size (MB): 4.48
Params size (MB): 58.16
Estimated Total Size (MB): 62.65
----------------------------------------------------------------
start converting conv layer
[0, 1, 4, 4]
torch.Size([64, 3, 3, 3])
[0, 8, 4, 4]
torch.Size([64, 8, 3, 3])
[0, 8, 2, 2]
torch.Size([128, 8, 3, 3])
[0, 16, 2, 2]
torch.Size([128, 8, 3, 3])
[0, 16, 1, 1]
torch.Size([256, 8, 3, 3])
[0, 32, 1, 1]
torch.Size([256, 8, 3, 3])
[0, 32, 1, 1]
torch.Size([256, 8, 3, 3])
[0, 32, 1, 1]
torch.Size([512, 8, 3, 3])
[0, 64, 1, 1]
torch.Size([512, 8, 3, 3])
[0, 64, 1, 1]
torch.Size([512, 8, 3, 3])
[0, 64, 1, 1]
torch.Size([512, 8, 3, 3])
[0, 64, 1, 1]
torch.Size([512, 8, 3, 3])
[0, 64, 1, 1]
torch.Size([512, 8, 3, 3])
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 64, 32, 32]           1,792
              ReLU-2           [-1, 64, 32, 32]               0
            Conv2d-3           [-1, 64, 32, 32]          36,928
              ReLU-4           [-1, 64, 32, 32]               0
         MaxPool2d-5           [-1, 64, 16, 16]               0
            Conv2d-6          [-1, 128, 16, 16]          73,856
              ReLU-7          [-1, 128, 16, 16]               0
            Conv2d-8          [-1, 128, 16, 16]         147,584
              ReLU-9          [-1, 128, 16, 16]               0
        MaxPool2d-10            [-1, 128, 8, 8]               0
           Conv2d-11            [-1, 256, 8, 8]         295,168
             ReLU-12            [-1, 256, 8, 8]               0
           Conv2d-13            [-1, 256, 8, 8]         590,080
             ReLU-14            [-1, 256, 8, 8]               0
           Conv2d-15            [-1, 256, 8, 8]         590,080
             ReLU-16            [-1, 256, 8, 8]               0
        MaxPool2d-17            [-1, 256, 4, 4]               0
           Conv2d-18            [-1, 512, 4, 4]       1,180,160
             ReLU-19            [-1, 512, 4, 4]               0
           Conv2d-20            [-1, 512, 4, 4]       2,359,808
             ReLU-21            [-1, 512, 4, 4]               0
           Conv2d-22            [-1, 512, 4, 4]       2,359,808
             ReLU-23            [-1, 512, 4, 4]               0
        MaxPool2d-24            [-1, 512, 2, 2]               0
           Conv2d-25            [-1, 512, 2, 2]       2,359,808
             ReLU-26            [-1, 512, 2, 2]               0
           Conv2d-27            [-1, 512, 2, 2]       2,359,808
             ReLU-28            [-1, 512, 2, 2]               0
           Conv2d-29            [-1, 512, 2, 2]       2,359,808
             ReLU-30            [-1, 512, 2, 2]               0
        MaxPool2d-31            [-1, 512, 1, 1]               0
          Dropout-32                  [-1, 512]               0
           Linear-33                  [-1, 512]         262,656
             ReLU-34                  [-1, 512]               0
          Dropout-35                  [-1, 512]               0
           Linear-36                  [-1, 512]         262,656
             ReLU-37                  [-1, 512]               0
           Linear-38                   [-1, 10]           5,130
================================================================
Total params: 15,245,130
Trainable params: 15,245,130
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.01
Forward/backward pass size (MB): 4.48
Params size (MB): 58.16
Estimated Total Size (MB): 62.65
----------------------------------------------------------------
start converting conv layer
split group [0, 1, 4, 4]
torch.Size([64, 3, 3, 3])
split group [0, 8, 4, 4]
torch.Size([64, 8, 3, 3])
split group [0, 8, 2, 2]
torch.Size([128, 8, 3, 3])
split group [0, 16, 2, 2]
torch.Size([128, 8, 3, 3])
split group [0, 16, 1, 1]
torch.Size([256, 8, 3, 3])
split group [0, 32, 1, 1]
torch.Size([256, 8, 3, 3])
split group [0, 32, 1, 1]
torch.Size([256, 8, 3, 3])
split group [0, 32, 1, 1]
torch.Size([512, 8, 3, 3])
split group [0, 64, 1, 1]
torch.Size([512, 8, 3, 3])
split group [0, 64, 1, 1]
torch.Size([512, 8, 3, 3])
split group [0, 64, 1, 1]
torch.Size([512, 8, 3, 3])
split group [0, 64, 1, 1]
torch.Size([512, 8, 3, 3])
split group [0, 64, 1, 1]
torch.Size([512, 8, 3, 3])
Sequential(
  (0): Block_Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (1): ReLU(inplace=True)
  (2): Block_Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)
  (3): ReLU(inplace=True)
  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (5): Block_Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)
  (6): ReLU(inplace=True)
  (7): Block_Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16)
  (8): ReLU(inplace=True)
  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (10): Block_Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16)
  (11): ReLU(inplace=True)
  (12): Block_Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)
  (13): ReLU(inplace=True)
  (14): Block_Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)
  (15): ReLU(inplace=True)
  (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (17): Block_Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)
  (18): ReLU(inplace=True)
  (19): Block_Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
  (20): ReLU(inplace=True)
  (21): Block_Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
  (22): ReLU(inplace=True)
  (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (24): Block_Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
  (25): ReLU(inplace=True)
  (26): Block_Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
  (27): ReLU(inplace=True)
  (28): Block_Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
  (29): ReLU(inplace=True)
  (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (31): Reshape()
  (32): Dropout(p=0.5, inplace=False)
  (33): Linear(in_features=512, out_features=512, bias=True)
  (34): ReLU(inplace=True)
  (35): Dropout(p=0.5, inplace=False)
  (36): Linear(in_features=512, out_features=512, bias=True)
  (37): ReLU(inplace=True)
  (38): Linear(in_features=512, out_features=10, bias=True)
)
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
      Block_Conv2d-1           [-1, 64, 32, 32]           1,792
              ReLU-2           [-1, 64, 32, 32]               0
      Block_Conv2d-3           [-1, 64, 32, 32]           4,672
              ReLU-4           [-1, 64, 32, 32]               0
         MaxPool2d-5           [-1, 64, 16, 16]               0
      Block_Conv2d-6          [-1, 128, 16, 16]           9,344
              ReLU-7          [-1, 128, 16, 16]               0
      Block_Conv2d-8          [-1, 128, 16, 16]           9,344
              ReLU-9          [-1, 128, 16, 16]               0
        MaxPool2d-10            [-1, 128, 8, 8]               0
     Block_Conv2d-11            [-1, 256, 8, 8]          18,688
             ReLU-12            [-1, 256, 8, 8]               0
     Block_Conv2d-13            [-1, 256, 8, 8]          18,688
             ReLU-14            [-1, 256, 8, 8]               0
     Block_Conv2d-15            [-1, 256, 8, 8]          18,688
             ReLU-16            [-1, 256, 8, 8]               0
        MaxPool2d-17            [-1, 256, 4, 4]               0
     Block_Conv2d-18            [-1, 512, 4, 4]          37,376
             ReLU-19            [-1, 512, 4, 4]               0
     Block_Conv2d-20            [-1, 512, 4, 4]          37,376
             ReLU-21            [-1, 512, 4, 4]               0
     Block_Conv2d-22            [-1, 512, 4, 4]          37,376
             ReLU-23            [-1, 512, 4, 4]               0
        MaxPool2d-24            [-1, 512, 2, 2]               0
     Block_Conv2d-25            [-1, 512, 2, 2]          37,376
             ReLU-26            [-1, 512, 2, 2]               0
     Block_Conv2d-27            [-1, 512, 2, 2]          37,376
             ReLU-28            [-1, 512, 2, 2]               0
     Block_Conv2d-29            [-1, 512, 2, 2]          37,376
             ReLU-30            [-1, 512, 2, 2]               0
        MaxPool2d-31            [-1, 512, 1, 1]               0
          Reshape-32                  [-1, 512]               0
          Dropout-33                  [-1, 512]               0
           Linear-34                  [-1, 512]         262,656
             ReLU-35                  [-1, 512]               0
          Dropout-36                  [-1, 512]               0
           Linear-37                  [-1, 512]         262,656
             ReLU-38                  [-1, 512]               0
           Linear-39                   [-1, 10]           5,130
================================================================
Total params: 835,914
Trainable params: 835,914
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.01
Forward/backward pass size (MB): 4.48
Params size (MB): 3.19
Estimated Total Size (MB): 7.68
----------------------------------------------------------------
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 64, 32, 32]           1,792
              ReLU-2           [-1, 64, 32, 32]               0
            Conv2d-3           [-1, 64, 32, 32]          36,928
              ReLU-4           [-1, 64, 32, 32]               0
         MaxPool2d-5           [-1, 64, 16, 16]               0
            Conv2d-6          [-1, 128, 16, 16]          73,856
              ReLU-7          [-1, 128, 16, 16]               0
            Conv2d-8          [-1, 128, 16, 16]         147,584
              ReLU-9          [-1, 128, 16, 16]               0
        MaxPool2d-10            [-1, 128, 8, 8]               0
           Conv2d-11            [-1, 256, 8, 8]         295,168
             ReLU-12            [-1, 256, 8, 8]               0
           Conv2d-13            [-1, 256, 8, 8]         590,080
             ReLU-14            [-1, 256, 8, 8]               0
           Conv2d-15            [-1, 256, 8, 8]         590,080
             ReLU-16            [-1, 256, 8, 8]               0
        MaxPool2d-17            [-1, 256, 4, 4]               0
           Conv2d-18            [-1, 512, 4, 4]       1,180,160
             ReLU-19            [-1, 512, 4, 4]               0
           Conv2d-20            [-1, 512, 4, 4]       2,359,808
             ReLU-21            [-1, 512, 4, 4]               0
           Conv2d-22            [-1, 512, 4, 4]       2,359,808
             ReLU-23            [-1, 512, 4, 4]               0
        MaxPool2d-24            [-1, 512, 2, 2]               0
           Conv2d-25            [-1, 512, 2, 2]       2,359,808
             ReLU-26            [-1, 512, 2, 2]               0
           Conv2d-27            [-1, 512, 2, 2]       2,359,808
             ReLU-28            [-1, 512, 2, 2]               0
           Conv2d-29            [-1, 512, 2, 2]       2,359,808
             ReLU-30            [-1, 512, 2, 2]               0
        MaxPool2d-31            [-1, 512, 1, 1]               0
          Dropout-32                  [-1, 512]               0
           Linear-33                  [-1, 512]         262,656
             ReLU-34                  [-1, 512]               0
          Dropout-35                  [-1, 512]               0
           Linear-36                  [-1, 512]         262,656
             ReLU-37                  [-1, 512]               0
           Linear-38                   [-1, 10]           5,130
================================================================
Total params: 15,245,130
Trainable params: 15,245,130
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.01
Forward/backward pass size (MB): 4.48
Params size (MB): 58.16
Estimated Total Size (MB): 62.65
----------------------------------------------------------------
start converting conv layer
split group [0, 1, 4, 4]
torch.Size([64, 3, 3, 3])
split group [0, 8, 4, 4]
torch.Size([64, 8, 3, 3])
split group [0, 8, 2, 2]
torch.Size([128, 8, 3, 3])
split group [0, 16, 2, 2]
torch.Size([128, 8, 3, 3])
split group [0, 16, 1, 1]
torch.Size([256, 8, 3, 3])
split group [0, 32, 1, 1]
torch.Size([256, 8, 3, 3])
split group [0, 32, 1, 1]
torch.Size([256, 8, 3, 3])
split group [0, 32, 1, 1]
torch.Size([512, 8, 3, 3])
split group [0, 64, 1, 1]
torch.Size([512, 8, 3, 3])
split group [0, 64, 1, 1]
torch.Size([512, 8, 3, 3])
split group [0, 64, 1, 1]
torch.Size([512, 8, 3, 3])
split group [0, 64, 1, 1]
torch.Size([512, 8, 3, 3])
split group [0, 64, 1, 1]
torch.Size([512, 8, 3, 3])
Sequential(
  (0): Block_Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (1): ReLU(inplace=True)
  (2): Block_Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)
  (3): ReLU(inplace=True)
  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (5): Block_Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)
  (6): ReLU(inplace=True)
  (7): Block_Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16)
  (8): ReLU(inplace=True)
  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (10): Block_Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16)
  (11): ReLU(inplace=True)
  (12): Block_Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)
  (13): ReLU(inplace=True)
  (14): Block_Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)
  (15): ReLU(inplace=True)
  (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (17): Block_Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)
  (18): ReLU(inplace=True)
  (19): Block_Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
  (20): ReLU(inplace=True)
  (21): Block_Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
  (22): ReLU(inplace=True)
  (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (24): Block_Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
  (25): ReLU(inplace=True)
  (26): Block_Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
  (27): ReLU(inplace=True)
  (28): Block_Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
  (29): ReLU(inplace=True)
  (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (31): Reshape()
  (32): Dropout(p=0.5, inplace=False)
  (33): Linear(in_features=512, out_features=512, bias=True)
  (34): ReLU(inplace=True)
  (35): Dropout(p=0.5, inplace=False)
  (36): Linear(in_features=512, out_features=512, bias=True)
  (37): ReLU(inplace=True)
  (38): Linear(in_features=512, out_features=10, bias=True)
)
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
      Block_Conv2d-1           [-1, 64, 32, 32]           1,792
              ReLU-2           [-1, 64, 32, 32]               0
      Block_Conv2d-3           [-1, 64, 32, 32]           4,672
              ReLU-4           [-1, 64, 32, 32]               0
         MaxPool2d-5           [-1, 64, 16, 16]               0
      Block_Conv2d-6          [-1, 128, 16, 16]           9,344
              ReLU-7          [-1, 128, 16, 16]               0
      Block_Conv2d-8          [-1, 128, 16, 16]           9,344
              ReLU-9          [-1, 128, 16, 16]               0
        MaxPool2d-10            [-1, 128, 8, 8]               0
     Block_Conv2d-11            [-1, 256, 8, 8]          18,688
             ReLU-12            [-1, 256, 8, 8]               0
     Block_Conv2d-13            [-1, 256, 8, 8]          18,688
             ReLU-14            [-1, 256, 8, 8]               0
     Block_Conv2d-15            [-1, 256, 8, 8]          18,688
             ReLU-16            [-1, 256, 8, 8]               0
        MaxPool2d-17            [-1, 256, 4, 4]               0
     Block_Conv2d-18            [-1, 512, 4, 4]          37,376
             ReLU-19            [-1, 512, 4, 4]               0
     Block_Conv2d-20            [-1, 512, 4, 4]          37,376
             ReLU-21            [-1, 512, 4, 4]               0
     Block_Conv2d-22            [-1, 512, 4, 4]          37,376
             ReLU-23            [-1, 512, 4, 4]               0
        MaxPool2d-24            [-1, 512, 2, 2]               0
     Block_Conv2d-25            [-1, 512, 2, 2]          37,376
             ReLU-26            [-1, 512, 2, 2]               0
     Block_Conv2d-27            [-1, 512, 2, 2]          37,376
             ReLU-28            [-1, 512, 2, 2]               0
     Block_Conv2d-29            [-1, 512, 2, 2]          37,376
             ReLU-30            [-1, 512, 2, 2]               0
        MaxPool2d-31            [-1, 512, 1, 1]               0
          Reshape-32                  [-1, 512]               0
          Dropout-33                  [-1, 512]               0
           Linear-34                  [-1, 512]         262,656
             ReLU-35                  [-1, 512]               0
          Dropout-36                  [-1, 512]               0
           Linear-37                  [-1, 512]         262,656
             ReLU-38                  [-1, 512]               0
           Linear-39                   [-1, 10]           5,130
================================================================
Total params: 835,914
Trainable params: 835,914
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.01
Forward/backward pass size (MB): 4.48
Params size (MB): 3.19
Estimated Total Size (MB): 7.68
----------------------------------------------------------------
Files already downloaded and verified
Epoch: [0][0/391]	Time 0.919 (0.919)	Data 0.564 (0.564)	Loss 2.3393 (2.3393)	Prec@1 10.938 (10.938)
Epoch: [0][20/391]	Time 0.073 (0.104)	Data 0.000 (0.027)	Loss 2.2694 (2.3102)	Prec@1 15.625 (11.384)
Epoch: [0][40/391]	Time 0.065 (0.085)	Data 0.000 (0.014)	Loss 2.2944 (2.3060)	Prec@1 7.031 (11.014)
Epoch: [0][60/391]	Time 0.058 (0.075)	Data 0.000 (0.010)	Loss 2.2622 (2.2965)	Prec@1 16.406 (11.642)
Epoch: [0][80/391]	Time 0.062 (0.071)	Data 0.000 (0.007)	Loss 2.1625 (2.2739)	Prec@1 17.969 (12.867)
Epoch: [0][100/391]	Time 0.071 (0.068)	Data 0.000 (0.006)	Loss 2.0604 (2.2472)	Prec@1 20.312 (14.047)
Epoch: [0][120/391]	Time 0.062 (0.066)	Data 0.000 (0.005)	Loss 2.1361 (2.2239)	Prec@1 18.750 (15.154)
Epoch: [0][140/391]	Time 0.054 (0.065)	Data 0.000 (0.004)	Loss 2.1439 (2.2089)	Prec@1 17.188 (15.608)
Epoch: [0][160/391]	Time 0.054 (0.065)	Data 0.000 (0.004)	Loss 2.0684 (2.1907)	Prec@1 20.312 (16.232)
Epoch: [0][180/391]	Time 0.055 (0.064)	Data 0.000 (0.003)	Loss 2.0086 (2.1707)	Prec@1 26.562 (16.765)
Epoch: [0][200/391]	Time 0.056 (0.063)	Data 0.000 (0.003)	Loss 2.0344 (2.1582)	Prec@1 23.438 (17.071)
Epoch: [0][220/391]	Time 0.057 (0.063)	Data 0.000 (0.003)	Loss 1.9123 (2.1471)	Prec@1 29.688 (17.435)
Epoch: [0][240/391]	Time 0.060 (0.062)	Data 0.000 (0.003)	Loss 1.9147 (2.1330)	Prec@1 21.875 (17.943)
Epoch: [0][260/391]	Time 0.071 (0.062)	Data 0.000 (0.002)	Loss 1.9263 (2.1198)	Prec@1 28.906 (18.451)
Epoch: [0][280/391]	Time 0.057 (0.062)	Data 0.000 (0.002)	Loss 1.8961 (2.1091)	Prec@1 25.000 (18.750)
Epoch: [0][300/391]	Time 0.064 (0.062)	Data 0.000 (0.002)	Loss 2.0285 (2.0966)	Prec@1 29.688 (19.199)
Epoch: [0][320/391]	Time 0.058 (0.062)	Data 0.000 (0.002)	Loss 1.8696 (2.0887)	Prec@1 22.656 (19.444)
Epoch: [0][340/391]	Time 0.060 (0.062)	Data 0.000 (0.002)	Loss 1.8851 (2.0757)	Prec@1 28.906 (19.891)
Epoch: [0][360/391]	Time 0.078 (0.062)	Data 0.000 (0.002)	Loss 1.9043 (2.0646)	Prec@1 29.688 (20.338)
Epoch: [0][380/391]	Time 0.058 (0.062)	Data 0.000 (0.002)	Loss 1.8800 (2.0525)	Prec@1 32.812 (20.774)
Test: [0/79]	Time 0.430 (0.430)	Loss 1.7488 (1.7488)	Prec@1 39.844 (39.844)
Test: [20/79]	Time 0.016 (0.036)	Loss 1.8435 (1.8079)	Prec@1 32.812 (33.185)
Test: [40/79]	Time 0.015 (0.026)	Loss 1.8336 (1.8143)	Prec@1 23.438 (32.146)
Test: [60/79]	Time 0.016 (0.023)	Loss 1.7415 (1.8057)	Prec@1 31.250 (31.980)
 * Prec@1 31.950
Epoch: [1][0/391]	Time 0.529 (0.529)	Data 0.431 (0.431)	Loss 1.8807 (1.8807)	Prec@1 28.125 (28.125)
Epoch: [1][20/391]	Time 0.058 (0.087)	Data 0.000 (0.021)	Loss 1.7035 (1.8227)	Prec@1 40.625 (30.246)
Epoch: [1][40/391]	Time 0.073 (0.075)	Data 0.000 (0.011)	Loss 1.7566 (1.7921)	Prec@1 26.562 (30.602)
Epoch: [1][60/391]	Time 0.061 (0.071)	Data 0.000 (0.007)	Loss 1.7669 (1.7905)	Prec@1 32.031 (30.584)
Epoch: [1][80/391]	Time 0.060 (0.068)	Data 0.000 (0.006)	Loss 1.7057 (1.7886)	Prec@1 28.906 (30.990)
Epoch: [1][100/391]	Time 0.056 (0.066)	Data 0.000 (0.005)	Loss 1.7647 (1.7930)	Prec@1 30.469 (31.080)
Epoch: [1][120/391]	Time 0.064 (0.065)	Data 0.000 (0.004)	Loss 1.9942 (1.8034)	Prec@1 25.000 (30.792)
Epoch: [1][140/391]	Time 0.059 (0.065)	Data 0.000 (0.003)	Loss 1.6345 (1.8020)	Prec@1 40.625 (30.951)
Epoch: [1][160/391]	Time 0.064 (0.064)	Data 0.000 (0.003)	Loss 1.6671 (1.7992)	Prec@1 34.375 (31.303)
Epoch: [1][180/391]	Time 0.057 (0.064)	Data 0.000 (0.003)	Loss 1.6390 (1.7973)	Prec@1 32.812 (31.263)
Epoch: [1][200/391]	Time 0.056 (0.063)	Data 0.000 (0.002)	Loss 1.7163 (1.7944)	Prec@1 42.969 (31.293)
Epoch: [1][220/391]	Time 0.057 (0.063)	Data 0.000 (0.002)	Loss 1.6779 (1.7875)	Prec@1 37.500 (31.582)
Epoch: [1][240/391]	Time 0.068 (0.063)	Data 0.000 (0.002)	Loss 1.6382 (1.7840)	Prec@1 39.062 (31.756)
Epoch: [1][260/391]	Time 0.064 (0.063)	Data 0.000 (0.002)	Loss 1.7278 (1.7752)	Prec@1 29.688 (32.046)
Epoch: [1][280/391]	Time 0.057 (0.063)	Data 0.000 (0.002)	Loss 1.6565 (1.7697)	Prec@1 31.250 (32.206)
Epoch: [1][300/391]	Time 0.057 (0.063)	Data 0.000 (0.002)	Loss 1.7675 (1.7651)	Prec@1 28.125 (32.345)
Epoch: [1][320/391]	Time 0.060 (0.062)	Data 0.000 (0.002)	Loss 1.6060 (1.7610)	Prec@1 36.719 (32.469)
Epoch: [1][340/391]	Time 0.060 (0.062)	Data 0.000 (0.002)	Loss 1.5974 (1.7554)	Prec@1 35.156 (32.581)
Epoch: [1][360/391]	Time 0.059 (0.062)	Data 0.000 (0.001)	Loss 1.5999 (1.7520)	Prec@1 37.500 (32.700)
Epoch: [1][380/391]	Time 0.074 (0.062)	Data 0.000 (0.001)	Loss 1.6010 (1.7459)	Prec@1 39.844 (32.895)
Test: [0/79]	Time 0.432 (0.432)	Loss 1.5967 (1.5967)	Prec@1 41.406 (41.406)
Test: [20/79]	Time 0.015 (0.037)	Loss 1.6266 (1.5872)	Prec@1 37.500 (39.174)
Test: [40/79]	Time 0.016 (0.026)	Loss 1.5928 (1.5765)	Prec@1 39.062 (39.539)
Test: [60/79]	Time 0.016 (0.023)	Loss 1.5331 (1.5793)	Prec@1 35.938 (39.293)
 * Prec@1 38.980
Epoch: [2][0/391]	Time 0.544 (0.544)	Data 0.456 (0.456)	Loss 1.6824 (1.6824)	Prec@1 35.938 (35.938)
Epoch: [2][20/391]	Time 0.059 (0.081)	Data 0.000 (0.022)	Loss 1.6574 (1.6683)	Prec@1 40.625 (36.719)
Epoch: [2][40/391]	Time 0.061 (0.070)	Data 0.000 (0.011)	Loss 1.6091 (1.6696)	Prec@1 35.938 (36.280)
Epoch: [2][60/391]	Time 0.058 (0.067)	Data 0.000 (0.008)	Loss 1.7748 (1.6693)	Prec@1 35.156 (35.912)
Epoch: [2][80/391]	Time 0.059 (0.065)	Data 0.000 (0.006)	Loss 1.5687 (1.6585)	Prec@1 42.188 (36.487)
Epoch: [2][100/391]	Time 0.055 (0.064)	Data 0.000 (0.005)	Loss 1.7107 (1.6520)	Prec@1 28.906 (36.773)
Epoch: [2][120/391]	Time 0.058 (0.064)	Data 0.000 (0.004)	Loss 1.5846 (1.6474)	Prec@1 32.812 (37.048)
Epoch: [2][140/391]	Time 0.058 (0.063)	Data 0.000 (0.003)	Loss 1.6261 (1.6481)	Prec@1 35.156 (36.974)
Epoch: [2][160/391]	Time 0.063 (0.063)	Data 0.000 (0.003)	Loss 1.6805 (1.6415)	Prec@1 33.594 (37.122)
Epoch: [2][180/391]	Time 0.056 (0.062)	Data 0.000 (0.003)	Loss 1.6278 (1.6352)	Prec@1 36.719 (37.289)
Epoch: [2][200/391]	Time 0.064 (0.062)	Data 0.000 (0.003)	Loss 1.8780 (1.6353)	Prec@1 32.031 (37.477)
Epoch: [2][220/391]	Time 0.064 (0.063)	Data 0.000 (0.002)	Loss 1.5303 (1.6303)	Prec@1 40.625 (37.588)
Epoch: [2][240/391]	Time 0.058 (0.063)	Data 0.000 (0.002)	Loss 1.5964 (1.6281)	Prec@1 35.938 (37.649)
Epoch: [2][260/391]	Time 0.061 (0.062)	Data 0.000 (0.002)	Loss 1.6506 (1.6275)	Prec@1 39.062 (37.724)
Epoch: [2][280/391]	Time 0.062 (0.063)	Data 0.000 (0.002)	Loss 1.4487 (1.6221)	Prec@1 44.531 (37.942)
Epoch: [2][300/391]	Time 0.063 (0.062)	Data 0.000 (0.002)	Loss 1.4947 (1.6167)	Prec@1 42.188 (38.128)
Epoch: [2][320/391]	Time 0.058 (0.062)	Data 0.000 (0.002)	Loss 1.4928 (1.6113)	Prec@1 44.531 (38.371)
Epoch: [2][340/391]	Time 0.064 (0.062)	Data 0.000 (0.002)	Loss 1.5944 (1.6074)	Prec@1 40.625 (38.577)
Epoch: [2][360/391]	Time 0.060 (0.062)	Data 0.000 (0.002)	Loss 1.3568 (1.6012)	Prec@1 46.875 (38.803)
Epoch: [2][380/391]	Time 0.056 (0.062)	Data 0.000 (0.001)	Loss 1.7433 (1.5993)	Prec@1 35.938 (38.886)
Test: [0/79]	Time 0.409 (0.409)	Loss 1.3860 (1.3860)	Prec@1 45.312 (45.312)
Test: [20/79]	Time 0.018 (0.035)	Loss 1.4560 (1.4290)	Prec@1 42.188 (44.680)
Test: [40/79]	Time 0.016 (0.026)	Loss 1.4352 (1.4259)	Prec@1 46.875 (44.798)
Test: [60/79]	Time 0.016 (0.023)	Loss 1.3288 (1.4256)	Prec@1 50.000 (44.685)
 * Prec@1 44.500
Epoch: [3][0/391]	Time 0.536 (0.536)	Data 0.420 (0.420)	Loss 1.5324 (1.5324)	Prec@1 42.969 (42.969)
Epoch: [3][20/391]	Time 0.055 (0.081)	Data 0.000 (0.020)	Loss 1.3107 (1.4906)	Prec@1 49.219 (43.118)